# DocStringAgent

An **AI-powered Python docstring generation agent** that automatically generates clean, Google-style docstrings for your Python code. Paste code, upload a file, or point to a server path â€” the agent analyses your code with AST introspection, calls an LLM, validates the output, and self-corrects up to 2 times until the docstring is accurate.

> ğŸ† **Hackathon Winner** â€” Won at the **Epoch x Nasiko Agentic AI Bootcamp** held at **IIIT Sricity**, February 2026.

---

## Agent Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Code â”‚â”€â”€â”€â”€â–¶â”‚  AST Analyzer â”‚â”€â”€â”€â”€â–¶â”‚  LLM Generation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚   Validator      â”‚
                                           â”‚  (rule-based)    â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Violations found?   â”‚
                                        â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                                          Yes              No
                                            â”‚               â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Correction Loopâ”‚  â”‚  Insert     â”‚
                                  â”‚ (max 2 passes) â”‚  â”‚  Docstring  â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                     back to LLM
```

### Core Pipeline

1. **AST Analysis** (`app/tools.py`) â€” Statically inspects each function/class to extract:
   - Parameters and type annotations
   - Explicit `raise` statements
   - Built-in implied exceptions (e.g. `open()` â†’ `FileNotFoundError`, `dict[key]` â†’ `KeyError`)
   - `async` / generator (`yield`) detection
   - Mutable default arguments (`list`, `dict`, `set`)

2. **LLM Prompting** (`app/agents.py`) â€” Sends the source code and analysis results to an LLM with a strict system prompt enforcing Google-style rules. The prompt forbids hallucination: the LLM may only document behaviour that is **explicitly present** in the code.

3. **Validation** (`app/tools.py`) â€” Checks the generated docstring against the AST analysis:
   - Has `Yields:` for generators? Has `Returns:` for non-generators?
   - Does the `Raises:` section only list actually-raised exceptions?
   - Is there a `Warning:` for mutable defaults?
   - Does it mention coroutine behaviour for async functions?

4. **Self-Correction Loop** â€” If validation fails, the agent sends the violations back to the LLM with the original source and AST data, requesting a corrected docstring. This repeats for up to **2 correction passes**.

5. **Docstring Insertion** â€” The validated docstring is spliced into the source code at the correct position, preserving indentation, decorators, and surrounding code.

### Multi-Model Support

| Provider | Models | Notes |
|----------|--------|-------|
| **Gemini** (cloud) | `gemini-2.0-flash`, `gemini-2.5-flash`, `gemini-2.5-pro` | Requires `GEMINI_API_KEY` in `.env` |
| **Ollama** (local) | Auto-detected from running Ollama server | No API key needed; runs fully offline |

The UI automatically discovers local Ollama models on startup and offers them alongside cloud models.

---

## Usage Instructions

### Prerequisites

- **Python 3.13+**
- **[uv](https://docs.astral.sh/uv/)** package manager
- A **Gemini API key** (for cloud models) and/or a running **Ollama** instance (for local models)

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/DocStringAgent.git
cd DocStringAgent

# Install dependencies
uv sync
uv venv

# Edit .env and add your GEMINI_API_KEY
```

### Web UI

```bash
uv run -m src serve
# Open http://localhost:8000
```

The web interface provides three input methods:
- **Paste Code** â€” paste Python code directly
- **Upload File** â€” drag & drop or browse for a `.py` file
- **Server Path** â€” enter a file or directory path on the server

Select a model from the dropdown (Gemini cloud or local Ollama), click **Generate Docstrings**, and view results in Modified / Original / Side-by-Side views.

### CLI

```bash
# Generate docstrings for a single file
uv run -m src generate path/to/file.py

# Generate and overwrite the file in-place
uv run -m src generate path/to/file.py --overwrite

# List available models
uv run -m src models
```

### Docker

You can also run the application using Docker:

```bash
# Using Docker Compose (Recommended)
docker compose up --build

# Using Docker directly
docker build -t docstring-agent .
docker run -p 8000:8000 docstring-agent
```

### Project Structure

```
DocStringAgent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __main__.py      # CLI commands (generate, serve, models)
â”‚   â”œâ”€â”€ agents.py        # LLM prompting, validation loop, docstring insertion
â”‚   â”œâ”€â”€ config.py        # Configuration constants, env loading
â”‚   â”œâ”€â”€ models.py        # LLM factory (Ollama/Gemini), model discovery
â”‚   â”œâ”€â”€ server.py        # FastAPI backend, API routes
â”‚   â””â”€â”€ tools.py         # AST analysis, validation rules
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html       # Web UI
â”‚   â”œâ”€â”€ styles.css       # Dark-mode stylesheet
â”‚   â””â”€â”€ app.js           # Frontend logic
â”œâ”€â”€ Dockerfile           # Docker build instructions
â”œâ”€â”€ docker-compose.yaml  # Docker Compose configuration
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env                 # API keys (not committed)
```

---

## Assumptions & Limitations

### Assumptions

- Input code is **syntactically valid Python**. The agent will reject code that fails `ast.parse()`.
- For cloud models, a valid **`GEMINI_API_KEY`** is set in the `.env` file.
- For local models, **Ollama is running** and accessible at `http://localhost:11434` (configurable via `OLLAMA_BASE_URL`).
- Functions and classes that **already have docstrings** are skipped (no overwriting of existing docs).

### Limitations

- **No cross-function context** â€” the agent documents each function/class in isolation. It does not trace call chains or understand broader application context.
- **Correction ceiling** â€” the self-correction loop is capped at **2 passes**. Complex or ambiguous functions may still produce imperfect docstrings after exhausting retries.
- **Single-file scope** â€” the agent processes one file at a time (directory mode iterates files individually). It does not resolve cross-file imports or dependencies.
- **No type inference** â€” if a parameter lacks type annotations, the agent describes it by usage rather than guessing a type. This is by design to avoid hallucination, but means less specific docs for untyped code.
- **LLM variability** â€” output quality depends on the chosen model. Larger models (e.g. Gemini 2.5 Pro, Qwen 14B) generally produce better results than smaller ones.
- **Rate limits** â€” cloud models (Gemini) are subject to API rate limits and quotas. Processing large directories may hit these limits.

---

## License

MIT
